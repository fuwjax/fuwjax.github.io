---
layout: single
title: Look Ma, No NaN
date: '2007-01-02T14:34:00.000-06:00'
tags: 
modified_time: '2007-01-05T23:27:21.053-06:00'
blogger_id: tag:blogger.com,1999:blog-7551548.post-6394842070747376024
---

It's a very common misconception that the number 2 and the symbol 2 are the same thing. If I have 2 apples, it would be somewhat tedious to make them look like the symbol 2, and then it may even be debatable that I no longer have 2 apples, but instead 2 apples worth of mashed up plant matter.<br /><br />Now, we have long had a symbol for dividing a positive number by zero, and that symbol is positive infinity (more precisely it's the limit of k/x for a positive integer k as x goes to zero from the positive side). It turns out that even though we've had this symbol for a good long time, we haven't made it an official member of the group of symbols we always use. In other words, even though it is possible to wind up with the symbol infinity as a result of a limit operation, we don't allow operations on the symbol infinity directly (mathematically speaking, you are not allowed to write infinity + 1 = infinity, even though most of us would accept this statement to be true). This is mostly because, as teachers have taught for centuries, infinity does not exist in the same way that the number 2 exists, therefore adding a number to something which doesn't really exist in the same way doesn't really have any meaning. But this is somewhat non-sensical, since the symbol infinity exists in the same way that the symbol 2 exists, and therefore, operations on that symbol could be defined (and probably would have been if anyone had actually thought them useful)<br /><br />While we have had a symbol for dividing a negative number by zero, namely negative infinity, no one has ever, to my knowledge, concocted a symbol for dividing zero by zero, aside from the good folks at IEEE who just called it "Not a Number" or NaN. But this is not mathematics, this is error handling. NaN is inheirently painful; it does not equal itself, so it is tedious to test for a NaN value. Moreover,<br /><br />What is most interesting to me as a mathematician is that if you do bother to define this utterly useless concept with a symbol, which Anderson calls nullity, you are free to allow it and positive and negative infinity to be admitted to the set of symbols on which you have defined operations. So by explicitly admitting nullity and tossing in a dozen new axioms, you gain completeness over all the standard operations.<br /><br />Let me reiterate: it is not so much defining a new symbol, namely nullity, for dividing zero by zero, as it is constructing a set of axioms that induct this new symbol along with the symbols for positive infinity and negative infinity into  the set of symbols over which our standard operations are defined, thereby making those operations complete over the set of symbols. This is very powerful. This means you are explicitly allowed to write infinity + 1 = infinity. For that matter, you're even allowed to say things like 0 x infinity = nullity and infinity - infinity = nullity.<br /><br />So, even though it is attempting to correct for the exact same issues as the IEEE NaN symbol, it is in essence claiming that dividing by zero, or operating on infinity, will guarantee you a result which is a symbol that can continue to be operated on. IEEE just breaks (that's not entirely fair, it may break, which in my opinion is a far worse case) when you operate on NaN; its operations are not complete over the set of symbols.<br /><br />Now, in terms of computing ability, the IEEE spec has some absurd number of special symbols. positive and negative infinity, min and max values, tons of various NaN codes (of which some are breaking and some are not), and my absolute favorite symbol "-0". Anderson's specification needs only 3 special symbols. That's huge, well, tiny, by way of comparison.<br /><br />Will you still have to check for NaN/Nullity? Yes, of course. It's always amused me that people blindly go around trusting the results of IEEE calculations just because they thought there wasn't any way to get a NaN result. What's interesting to me is that with Anderson you will always get a consistent result that you can validate with a single comparison. You can't do that in IEEE.<br /><br />Moreover, computers have always had issues with upper and lower bounds on the integers. Now, most of us have learned to take advantages of these issues, but that doesn't make them right. With Anderson's scheme, at some point on a computer, if you add one to a very large number, you'll wind up at positive infinity, and then when you keep adding, you won't leave infinity. Right now on most hardware implemented integers, you'll add one to a very large number and wind up at MAXINT, then add one again, and you'll wind up at MININT with an overflow flag set. Some architectures break on overflow, some don't. You always check for overflow on every addition, don't you?<br /><br />Frankly I'm a bit shocked at the reaction he seems to be getting around here. There are obviously a bunch of very intelligent people on this site who are using a web news article as their sole source when the published papers are available for free.  I'm not saying this guy is not a quack; I'm just saying it might be wise to get a few facts straight before burning his digital effigy.